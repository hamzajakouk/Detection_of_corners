{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4f5LbQklZRn",
        "outputId": "d4915688-81cc-4b85-c92b-374449e6ca06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# prompt: connect google drive to google collab\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load your dataset from a file\n",
        "with open('/content/drive/MyDrive/est/updated_image_data.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "# The train_data and val_data now contain the split data\n"
      ],
      "metadata": {
        "id": "5TSgX5ojIMUw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "def preprocess_image(image_path, target_size):\n",
        "    img = load_img(image_path, target_size=target_size)\n",
        "    img = img_to_array(img)\n",
        "    img = img / 255.0  # normalize pixel values\n",
        "    return img\n",
        "\n",
        "def process_dataset(data, image_folder, target_size):\n",
        "    images = []\n",
        "    keypoints = []\n",
        "    for entry in data:\n",
        "        img_path = f\"{image_folder}/{entry['filename']}\"\n",
        "        img = preprocess_image(img_path, target_size)\n",
        "        images.append(img)\n",
        "\n",
        "        kp = [kp['coordinates'] for kp in entry['objects']]\n",
        "        kp = np.array(kp).flatten()  # flatten keypoints to a single array\n",
        "        kp = kp / np.array([entry['width'], entry['height']] * 4)  # normalize keypoints\n",
        "        keypoints.append(kp)\n",
        "\n",
        "    return np.array(images), np.array(keypoints)\n",
        "\n",
        "# Load dataset\n",
        "with open('/content/drive/MyDrive/est/updated_image_data.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Split the dataset\n",
        "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Process dataset\n",
        "image_folder = '/content/drive/MyDrive/est/data'  # path to the folder containing images\n",
        "target_size = (224, 224)  # desired size of the images\n",
        "train_images, train_keypoints = process_dataset(train_data, image_folder, target_size)\n",
        "val_images, val_keypoints = process_dataset(val_data, image_folder, target_size)\n",
        "\n",
        "# Define the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
        "    MaxPooling2D(2, 2),\n",
        "    # Additional layers...\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(8)  # 8 neurons for 4 keypoints (x, y) coordinates\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_images, train_keypoints, epochs=10, validation_data=(val_images, val_keypoints))\n",
        "\n",
        "# Save the model\n",
        "model.save('keypoint_model.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kFX1YA1P-k-",
        "outputId": "78488862-c064-4f8f-8dd5-6cd62e8dafdb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "5/5 [==============================] - 17s 3s/step - loss: 1143.0323 - val_loss: 65.5788\n",
            "Epoch 2/10\n",
            "5/5 [==============================] - 20s 3s/step - loss: 35.4380 - val_loss: 10.5656\n",
            "Epoch 3/10\n",
            "5/5 [==============================] - 12s 3s/step - loss: 5.6147 - val_loss: 5.7208\n",
            "Epoch 4/10\n",
            "5/5 [==============================] - 10s 2s/step - loss: 3.2139 - val_loss: 2.9455\n",
            "Epoch 5/10\n",
            "5/5 [==============================] - 11s 2s/step - loss: 2.4239 - val_loss: 1.0842\n",
            "Epoch 6/10\n",
            "5/5 [==============================] - 12s 3s/step - loss: 0.7717 - val_loss: 0.6098\n",
            "Epoch 7/10\n",
            "5/5 [==============================] - 10s 2s/step - loss: 0.4685 - val_loss: 0.3735\n",
            "Epoch 8/10\n",
            "5/5 [==============================] - 12s 2s/step - loss: 0.3385 - val_loss: 0.3671\n",
            "Epoch 9/10\n",
            "5/5 [==============================] - 12s 3s/step - loss: 0.3370 - val_loss: 0.3669\n",
            "Epoch 10/10\n",
            "5/5 [==============================] - 10s 2s/step - loss: 0.3367 - val_loss: 0.3666\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "import numpy as np\n",
        "\n",
        "def preprocess_image(image_path, target_size):\n",
        "    img = load_img(image_path, target_size=target_size)\n",
        "    img = img_to_array(img)\n",
        "    img = img / 255.0  # Normalize pixel values\n",
        "    return img\n",
        "\n",
        "def predict_keypoints(model, image_path, image_size):\n",
        "    processed_image = preprocess_image(image_path, image_size)\n",
        "    processed_image = np.expand_dims(processed_image, axis=0)  # Add batch dimension\n",
        "\n",
        "    predicted_keypoints = model.predict(processed_image)[0]\n",
        "    # Rescale keypoints back to original image size\n",
        "    predicted_keypoints = predicted_keypoints * np.array(image_size * 4)\n",
        "    return predicted_keypoints.reshape(-1, 2)  # Reshape to pairs of (x, y)\n",
        "\n",
        "# Load the trained model\n",
        "model = load_model('keypoint_model.h5')\n",
        "\n",
        "# Test the model on a new image\n",
        "test_image_path = '/content/drive/MyDrive/est/data/1.jpg'\n",
        "predicted_keypoints = predict_keypoints(model, test_image_path, (224, 224))\n",
        "\n",
        "print(\"Predicted Keypoints:\", predicted_keypoints)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jS3TNdwWQ-EO",
        "outputId": "e2df8d1e-135f-442a-9e84-a4149a447319"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 128ms/step\n",
            "Predicted Keypoints: [[ 0.85691219  0.30269648]\n",
            " [-0.07380011  1.37918596]\n",
            " [-1.38342744 -1.53681724]\n",
            " [-0.74362516 -0.64230306]]\n"
          ]
        }
      ]
    }
  ]
}